import os
import sys
import warnings
import subprocess
import multiprocessing
from pathlib import Path
import psutil
import torch
import pandas as pd
import numpy as np
import json
import time
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import threading
import re
from typing import List, Dict, Any, Tuple
import pickle
from tqdm import tqdm
import gc

# é¢„è®¾ç½® HuggingFace é•œåƒç«¯ç‚¹ï¼ˆéœ€åœ¨ç›¸å…³åº“å¯¼å…¥å‰ç”Ÿæ•ˆï¼‰
if os.environ.get('HF_ENDPOINT') is None:
    os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

from sentence_transformers import SentenceTransformer
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, 
    BitsAndBytesConfig
)
from peft import (
    LoraConfig, get_peft_model, TaskType,
    prepare_model_for_kbit_training
)

warnings.filterwarnings('ignore')

def configure_hf(mirror: str = None, offline: bool = False) -> None:
    try:
        if mirror:
            os.environ.setdefault('HF_ENDPOINT', mirror)
        if offline:
            os.environ.setdefault('HF_HUB_OFFLINE', '1')
            os.environ.setdefault('TRANSFORMERS_OFFLINE', '1')
        endpoint = os.environ.get('HF_ENDPOINT', 'https://huggingface.co')
        print(f"HuggingFace ç«¯ç‚¹: {endpoint} | ç¦»çº¿: {os.environ.get('HF_HUB_OFFLINE', '0')}")
    except Exception as e:
        print(f"é…ç½® HuggingFace é•œåƒå¤±è´¥: {e}")

configure_hf(mirror="https://hf-mirror.com")

CPU_COUNT = multiprocessing.cpu_count()
GPU_AVAILABLE = torch.cuda.is_available()
GPU_MEMORY_GB = torch.cuda.get_device_properties(0).total_memory / (1024**3) if GPU_AVAILABLE else 0

if GPU_AVAILABLE:
    print(f"GPU å¯ç”¨: {torch.cuda.get_device_name(0)}")
else:
    print("GPU ä¸å¯ç”¨")

LINUX_CONFIG = {
    "use_gpu": GPU_AVAILABLE,
    "device": "cuda" if GPU_AVAILABLE else "cpu",
    "num_workers": min(CPU_COUNT, 8),
    "prefetch_factor": 2,
    "pin_memory": GPU_AVAILABLE,
    "thread_pool_size": CPU_COUNT // 2,
    "process_pool_size": min(CPU_COUNT // 2, 4),
    "model": {"model_name": "ShengbinYue/LawLLM-7B", "max_length": 1024, "batch_size": 2, "use_4bit": True, "cpu_offload": False},
    "rag_config": {"embedding_model": "sentence-transformers/all-MiniLM-L6-v2", "top_k": 5, "similarity_threshold": 0.7, "chunk_size": 256, "chunk_overlap": 50, "index_type": "sklearn"},
    "lora_config": {"r": 8, "lora_alpha": 32, "lora_dropout": 0.1, "target_modules": ["q_proj", "v_proj"], "bias": "none", "task_type": "CAUSAL_LM"},
    "training_config": {"learning_rate": 1e-4, "num_epochs": 3, "warmup_steps": 100, "gradient_accumulation_steps": 2, "dataloader_num_workers": min(CPU_COUNT, 4), "fp16": GPU_AVAILABLE, "gradient_checkpointing": True}
}

from sklearn.metrics.pairwise import cosine_similarity

class LegalRAGSystem:
    def __init__(self, config=LINUX_CONFIG):
        self.config = config
        self.embedding_model = None
        self.knowledge_base = []
        self.embeddings = None
        self.index = None

    def load_embedding_model(self):
        print("æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹...")
        model_name = self.config['rag_config']['embedding_model']

        self.embedding_model = SentenceTransformer(
            model_name,
            device=self.config['device']
        )
        print(f"åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}")

    def load_index(self, path="legal_rag_index"):
        print(f"åŠ è½½RAGç´¢å¼•: {path}...")
        try:
            with open(f"{path}_knowledge.pkl", 'rb') as f:
                data = pickle.load(f)
                self.knowledge_base = data['knowledge_base']
                saved_faiss_available = data.get('faiss_available', True)
            
            self.embeddings = np.load(f"{path}_embeddings.npy")
            
            print("ä½¿ç”¨sklearnè¿›è¡Œå‘é‡æœç´¢")
            self.index = None
            
            self.load_embedding_model()
            print("RAGç´¢å¼•åŠ è½½å®Œæˆ")
            return True
        except Exception as e:
            print(f"æ— æ³•åŠ è½½ç´¢å¼•: {e}")
            return False

    def search(self, query: str, top_k: int = None) -> List[Dict]:
        if top_k is None:
            top_k = self.config['rag_config']['top_k']
        
        query_embedding = self.embedding_model.encode(
            [query], normalize_embeddings=True, convert_to_numpy=True, device=self.config['device']
        )
        
        similarities = cosine_similarity(query_embedding.reshape(1, -1), self.embeddings)[0]
        top_indices = np.argsort(similarities)[::-1][:top_k]
        top_scores = similarities[top_indices]
        score_idx_pairs = list(zip(top_scores, top_indices))
        
        results = []
        for score, idx in score_idx_pairs:
            if idx >= len(self.knowledge_base):
                continue
            if score > self.config['rag_config']['similarity_threshold']:
                results.append({
                    'content': self.knowledge_base[idx]['content'],
                    'metadata': self.knowledge_base[idx]['metadata'],
                    'score': float(score),
                    'type': self.knowledge_base[idx]['type']
                })
        return results

class LegalLoRATrainer:
    def __init__(self, config=LINUX_CONFIG):
        self.config = config
        model_config = config.get('model')
        if not model_config:
            raise KeyError("æœªæ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹é…ç½®ï¼Œè¯·åœ¨é…ç½®ä¸­æä¾› 'model' å­—æ®µ")
        self.model_config = model_config
        self.lora_config = config['lora_config']
        self.tokenizer = None
        self.model = None
        self.peft_model = None

    def load_model_and_tokenizer(self, model_dir="./legal_lora_model"):
        print(f"åŠ è½½æ¨¡å‹å’Œtokenizer: {model_dir}...")
        device = self.config['device']

        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            model_kwargs = {
                'trust_remote_code': True,
                'device_map': 'auto' if device == 'cuda' else None,
                'torch_dtype': torch.float16 if device == 'cuda' else torch.float32,
            }

            if self.model_config['use_4bit'] and device == 'cuda':
                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                )
                model_kwargs['quantization_config'] = bnb_config
                print("4-bité‡åŒ–å·²å¯ç”¨")
            
            base_model_name = self.model_config['model_name']
            base_model = AutoModelForCausalLM.from_pretrained(base_model_name, **model_kwargs)
            
            self.peft_model = get_peft_model(base_model, LoraConfig.from_pretrained(model_dir))
            
            print(f"PEFTæ¨¡å‹åŠ è½½æˆåŠŸ: {model_dir}")

        except Exception as e:
            print(f"æ— æ³•åŠ è½½æ¨¡å‹: {e}")
            raise RuntimeError(f"Failed to load LoRA model: {e}")

class IntegratedLegalAnalyzer:
    def __init__(self, rag_system, lora_trainer, config=LINUX_CONFIG):
        self.rag_system = rag_system
        self.lora_trainer = lora_trainer
        self.config = config

    def analyze_clause(self, clause_text: str, detailed: bool = True) -> Dict[str, Any]:
        print(f"åˆ†ææ¡æ¬¾: {clause_text[:50]}...")
        start_time = time.time()
        rag_results = self.rag_system.search(clause_text, top_k=5)
        enhanced_context = self._build_enhanced_context(clause_text, rag_results)
        analysis = self._generate_analysis(enhanced_context)
        result = self._integrate_results(clause_text, rag_results, analysis, detailed)
        result['processing_time'] = time.time() - start_time
        print(f"åˆ†æå®Œæˆ. è€—æ—¶: {result['processing_time']:.2f}s")
        return result

    def _build_enhanced_context(self, clause_text: str, rag_results: List[Dict]) -> str:
        context = f"åˆ†ææ¡æ¬¾ï¼š{clause_text}\\n\\nå‚è€ƒï¼š\\n"
        for i, item in enumerate(rag_results[:2], 1):
            content = item['content'][:100]
            context += f"{i}. {content}...\\n"
        context += "\\nè¯·ç®€è¦åˆ†æé£é™©å’Œå»ºè®®ï¼š"
        return context

    def _generate_analysis(self, enhanced_context: str) -> str:
        try:
            inputs = self.lora_trainer.tokenizer(
                enhanced_context,
                return_tensors="pt",
                max_length=self.lora_trainer.model_config['max_length'],
                truncation=True,
                padding=True
            ).to(self.config['device'])

            with torch.no_grad():
                outputs = self.lora_trainer.peft_model.generate(
                    **inputs,
                    max_new_tokens=512,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    pad_token_id=self.lora_trainer.tokenizer.eos_token_id
                )
            
            input_length = inputs['input_ids'].shape[1]
            analysis = self.lora_trainer.tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True)
            return analysis.strip()
        except Exception as e:
            print(f"LoRAç”Ÿæˆåˆ†æå¤±è´¥: {e}")
            raise RuntimeError(f"LoRAæ¨¡å‹ç”Ÿæˆåˆ†æå¤±è´¥: {e}")

    def _integrate_results(self, clause_text: str, rag_results: List[Dict], analysis: str, detailed: bool) -> Dict[str, Any]:
        return {
            'input_clause': clause_text,
            'risk_level': self._assess_risk_level(clause_text, analysis),
            'analysis': analysis,
            'rag_sources': len(rag_results) if detailed else [],
        }

    def _assess_risk_level(self, clause_text: str, analysis: str) -> str:
        text_lower = (clause_text + analysis).lower()
        if any(keyword in text_lower for keyword in ['é«˜é£é™©', 'ä¸¥é‡', 'æ— æ•ˆ', 'è¿æ³•']):
            return "é«˜é£é™©"
        if any(keyword in text_lower for keyword in ['ä¸­ç­‰é£é™©', 'å»ºè®®ä¿®æ”¹', 'ä¸æ˜ç¡®', 'é£é™©']):
            return "ä¸­ç­‰é£é™©"
        return "ä½é£é™©"

class OptimizedContractAnalyzer:
    def __init__(self, legal_analyzer):
        self.legal_analyzer = legal_analyzer
        self.rag_system = legal_analyzer.rag_system
        self.lora_trainer = legal_analyzer.lora_trainer
        self.config = legal_analyzer.config
        self.performance_config = {
            'max_input_length': 600,
            'max_tokenizer_length': 400,
            'max_new_tokens': 128, 
            'rag_top_k': 3,
            'context_truncate_length': 100,
            'clause_min_length': 20,
            'paragraph_min_length': 30,
            'max_parallel_workers': 4
        }
        print("é«˜æ€§èƒ½åˆåŒåˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")

    def analyze_contract(self, contract_text: str, auto_extract: bool = True, performance_mode: str = 'balanced') -> Dict[str, Any]:
        print(f"åˆ†æåˆåŒ (æ¨¡å¼: {performance_mode})...")
        print(f"åˆåŒé•¿åº¦: {len(contract_text)} å­—ç¬¦")
        start_time = time.time()
        
        try:
            clauses = self._extract_contract_clauses(contract_text, auto_extract, performance_mode)
            print(f"å‘ç° {len(clauses)} ä¸ªæ¡æ¬¾.")
            
            if not clauses:
                return self._create_fallback_result("No clauses found.", start_time, "Clause extraction failed.")
            clause_results = self._batch_analyze_optimized(clauses, performance_mode)
            contract_analysis = self._generate_contract_summary_fast(clauses, clause_results)
            risk_assessment = self._assess_contract_risks_enhanced(clause_results)
            modification_suggestions = self._generate_smart_suggestions(clause_results)
            processing_time = time.time() - start_time
            result = self._build_comprehensive_result(
                contract_text, clauses, clause_results,
                contract_analysis, risk_assessment,
                modification_suggestions, processing_time,
                performance_mode
            )
            
            print(f"åˆåŒåˆ†æå®Œæˆï¼Œè€—æ—¶ {processing_time:.2f}s.")
            print(f"åˆ†æ {len(clauses)} ä¸ªæ¡æ¬¾ï¼Œå‘ç° {len(modification_suggestions)} ä¸ªæ”¹è¿›ç‚¹")
            
            return result
            
        except Exception as e:
            print(f"åˆåŒåˆ†æå¤±è´¥: {e}")
            return self._create_fallback_result(contract_text, start_time, str(e))

    def _extract_contract_clauses(self, contract_text: str, auto_extract: bool = True, performance_mode: str = 'balanced') -> List[str]:
        
        if not auto_extract:
            return [contract_text]
        
        clause_patterns = [
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+æ¡[ï¼š:]?',
            r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ã€ï¼\.]',
            r'\d+[\.ã€]',
            r'[ï¼ˆ(][ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ï¼‰)]',
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« ',
            r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+æ¡',
        ]
        
        
        clauses = []
        current_clause = ""
        lines = contract_text.split('\n')
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            is_new_clause = any(re.match(pattern, line) for pattern in clause_patterns)
            
            if is_new_clause and current_clause:
                clauses.append(current_clause.strip())
                current_clause = line
            else:
                if current_clause:
                    current_clause += '\n' + line
                else:
                    current_clause = line
        
        if current_clause:
            clauses.append(current_clause.strip())
        
        min_length = self.performance_config['clause_min_length']
            
        valid_clauses = [clause for clause in clauses if len(clause) >= min_length]
        
        if not valid_clauses or len(valid_clauses) <= 1:
            print("ä½¿ç”¨æ™ºèƒ½åˆ†å‰²æ¨¡å¼...")
            valid_clauses = self._smart_split_contract(contract_text)
        
        return valid_clauses

    def _smart_split_contract(self, contract_text: str) -> List[str]:
        
        keyword_patterns = [
            r'(å·¥ä½œæ—¶é—´|å·¥ä½œå†…å®¹|åŠ³åŠ¨æŠ¥é…¬|å·¥èµ„|è–ªé…¬|è¯•ç”¨æœŸ|åˆåŒæœŸé™|è¿çº¦|è§£é™¤|ç»ˆæ­¢|ä¿å¯†|ç«ä¸š|ç¤¾ä¼šä¿é™©|ç¦åˆ©|ä¼‘å‡|åŠ ç­|å¥–æƒ©|åŸ¹è®­|äº‰è®®|çº çº·|å…¶ä»–|é™„åˆ™)',
            r'([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]\s*[ã€ï¼\.])',
            r'(\d+\s*[ã€ï¼\.])',
            r'([ï¼ˆ(]\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+\s*[ï¼‰)])',
        ]
        
        clauses = []
        
        for pattern in keyword_patterns:
            parts = re.split(pattern, contract_text, flags=re.IGNORECASE)
            if len(parts) > 3:
                current_clause = ""
                for i, part in enumerate(parts):
                    if re.match(pattern, part, re.IGNORECASE):
                        if current_clause.strip():
                            clauses.append(current_clause.strip())
                        current_clause = part
                    else:
                        current_clause += part
                
                if current_clause.strip():
                    clauses.append(current_clause.strip())
                break
        
        if not clauses or len(clauses) <= 1:
            sentences = contract_text.split('ã€‚')
            current_clause = ""
            
            for sentence in sentences:
                sentence = sentence.strip()
                if not sentence:
                    continue
                
                current_clause += sentence + 'ã€‚'
                
                if len(current_clause) >= 100:
                    clauses.append(current_clause.strip())
                    current_clause = ""
            
            if current_clause.strip():
                clauses.append(current_clause.strip())
        
        if not clauses or len(clauses) <= 1:
            clauses = self._split_by_paragraphs_optimized(contract_text)
        
        min_length = 30 
        valid_clauses = [clause for clause in clauses if len(clause) >= min_length]
        
        return valid_clauses

    def _force_split_long_text(self, contract_text: str) -> List[str]:
        
        chunk_size = 300
        clauses = []
        
        words = contract_text.split()
        current_clause = ""
        
        for word in words:
            if len(current_clause + word) > chunk_size and current_clause:
                clauses.append(current_clause.strip())
                current_clause = word
            else:
                current_clause += " " + word if current_clause else word
        
        if current_clause.strip():
            clauses.append(current_clause.strip())
        
        return clauses if len(clauses) > 1 else [contract_text]

    def _split_by_paragraphs_optimized(self, contract_text: str) -> List[str]:
        
        paragraphs = contract_text.split('\n\n')
        min_length = self.performance_config['paragraph_min_length']
        valid_paragraphs = [
            para.strip() for para in paragraphs 
            if para.strip() and len(para.strip()) >= min_length
        ]
        
        if len(valid_paragraphs) < 3:
            lines = contract_text.split('\n')
            current_para = ""
            valid_paragraphs = []
            
            for line in lines:
                line = line.strip()
                if line:
                    current_para = current_para + '\n' + line if current_para else line
                    if len(current_para) > 200:
                        valid_paragraphs.append(current_para)
                        current_para = ""
            
            if current_para:
                valid_paragraphs.append(current_para)
        
        return valid_paragraphs

    def _batch_analyze_optimized(self, clauses: List[str], performance_mode: str) -> List[Dict]:
        results = []
        for i, clause in enumerate(clauses):
            try:
                result = self._analyze_single_clause_optimized(clause)
                result['clause_index'] = i
                results.append(result)
            except Exception as e:
                print(f"æ¡æ¬¾ {i} åˆ†æå¤±è´¥: {e}")
                results.append(self._create_fallback_clause_result(clause, i))
        return results
    
    def _analyze_single_clause_optimized(self, clause_text: str) -> Dict[str, Any]:
        start_time = time.time()
        
        rag_results = self.rag_system.search(clause_text, top_k=self.performance_config['rag_top_k'])
        
        enhanced_context = self._build_optimized_context(clause_text, rag_results)
        
        analysis = self._generate_analysis_optimized(enhanced_context)
        
        result = self._integrate_results_fast(clause_text, rag_results, analysis)
        result['processing_time'] = time.time() - start_time
        
        return result

    def _build_optimized_context(self, clause_text: str, rag_results: List[Dict]) -> str:
        context = f"è¯·åˆ†æä»¥ä¸‹åŠ³åŠ¨åˆåŒæ¡æ¬¾çš„åˆè§„é£é™©ï¼Œå¹¶ç»™å‡ºä¿®æ”¹å»ºè®®ï¼š\næ¡æ¬¾åŸæ–‡ï¼š'{clause_text}'\n\n"
        if rag_results:
            context += "å‚è€ƒä¿¡æ¯ï¼š\n"
            for item in rag_results:
                context += f"- {item['content']}\n"
        return context

    def _generate_analysis_optimized(self, enhanced_context: str) -> str:
        """LoRAæ¨¡å‹ä¸“ä¸šåˆ†æç”Ÿæˆ - çº¯AIé©±åŠ¨ï¼Œæ— å¤‡ç”¨æœºåˆ¶"""
        
        if not hasattr(self.lora_trainer, 'peft_model') or self.lora_trainer.peft_model is None:
            raise RuntimeError("LoRAæ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•è¿›è¡Œä¸“ä¸šåˆ†æ")

        structured_prompt = f"""
è¯·å¯¹ä»¥ä¸‹åŠ³åŠ¨åˆåŒæ¡æ¬¾è¿›è¡Œä¸“ä¸šçš„æ³•å¾‹é£é™©åˆ†æï¼ŒæŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š

{enhanced_context}

è¯·æŒ‰ä»¥ä¸‹ç»“æ„è¿›è¡Œåˆ†æï¼š
æ¡æ¬¾ç¼ºé™·åˆ†æï¼š[è¯¦ç»†è¯´æ˜æ¡æ¬¾å­˜åœ¨çš„æ³•å¾‹é£é™©å’Œé—®é¢˜]
ç›¸å…³æ¡ˆä¾‹ï¼š[æä¾›ç›¸å…³çš„æ³•å¾‹æ¡ˆä¾‹å’Œåˆ¤ä¾‹]
åˆ¤å†³ç»“æœï¼š[è¯´æ˜æ³•é™¢å¯¹ç±»ä¼¼æ¡ˆä¾‹çš„åˆ¤å†³å€¾å‘]
æ³•å¾‹ä¾æ®ï¼š[å¼•ç”¨ç›¸å…³çš„æ³•å¾‹æ¡æ–‡]
ä¿®æ”¹å»ºè®®ï¼š[æä¾›å…·ä½“çš„ä¿®æ”¹å»ºè®®]
é£é™©ç­‰çº§ï¼š[é«˜é£é™©/ä¸­ç­‰é£é™©/ä½é£é™©]
"""

        inputs = self.lora_trainer.tokenizer(
            structured_prompt,
            return_tensors="pt", 
            truncation=True, 
            max_length=self.performance_config['max_tokenizer_length']
        )
        
        device = self.config['device']
        inputs = {k: v.to(device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = self.lora_trainer.peft_model.generate(
                **inputs, 
                max_new_tokens=self.performance_config['max_new_tokens'],
                min_new_tokens=50,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1,
                pad_token_id=self.lora_trainer.tokenizer.pad_token_id or self.lora_trainer.tokenizer.eos_token_id,
                eos_token_id=self.lora_trainer.tokenizer.eos_token_id
            )
        
        input_length = inputs['input_ids'].shape[1]
        generated_tokens = outputs[0][input_length:]
        
        analysis = self.lora_trainer.tokenizer.decode(
            generated_tokens,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=True
        ).strip()
        
        if len(analysis) < 50:
            raise RuntimeError(f"LoRAæ¨¡å‹ç”Ÿæˆå†…å®¹è¿‡çŸ­ï¼ˆ{len(analysis)}å­—ç¬¦ï¼‰ï¼Œåˆ†æè´¨é‡ä¸ç¬¦åˆè¦æ±‚")
        
        if not any(keyword in analysis for keyword in ['åˆ†æ', 'å»ºè®®', 'é£é™©', 'æ³•å¾‹']):
            raise RuntimeError("LoRAæ¨¡å‹ç”Ÿæˆå†…å®¹ä¸åŒ…å«æ³•å¾‹åˆ†æå…³é”®ä¿¡æ¯ï¼Œåˆ†æå¤±è´¥")
        
        print(f"LoRAæ¨¡å‹æˆåŠŸç”Ÿæˆä¸“ä¸šåˆ†æï¼Œé•¿åº¦: {len(analysis)} å­—ç¬¦")
        return analysis

    def _integrate_results_fast(self, clause_text: str, rag_results: List[Dict], analysis: str) -> Dict[str, Any]:
        risk_level = self._assess_risk_level_fast(clause_text + analysis)
        parsed_info = self._parse_analysis_fast(analysis)
        result = {
            'input_clause': clause_text,
            'analysis': analysis,
            'rag_sources': len(rag_results),
            'risk_level': risk_level
        }
        result.update(parsed_info)
        return result

    def _assess_risk_level_fast(self, text: str) -> str:
        """åŸºäºAIåˆ†æç»“æœçš„æ™ºèƒ½é£é™©è¯„çº§"""
        text_lower = text.lower()
        
        if 'é£é™©ç­‰çº§ï¼šé«˜é£é™©' in text or 'é£é™©ç­‰çº§ï¼šé«˜' in text:
            return "é«˜é£é™©"
        elif 'é£é™©ç­‰çº§ï¼šä¸­ç­‰é£é™©' in text or 'é£é™©ç­‰çº§ï¼šä¸­' in text:
            return "ä¸­ç­‰é£é™©"
        elif 'é£é™©ç­‰çº§ï¼šä½é£é™©' in text or 'é£é™©ç­‰çº§ï¼šä½' in text:
            return "ä½é£é™©"
        
        high_risk_indicators = [
            'ä¸¥é‡è¿æ³•', 'æ— æ•ˆæ¡æ¬¾', 'æ˜¾å¤±å…¬å¹³', 'è¿åæ³•å¾‹', 'è´¥è¯‰', 'ä»²è£è´¥è¯‰',
            'è¶…å‡ºæ³•å®š', 'ä¸åˆæ³•', 'è¿çº¦é‡‘è¿‡é«˜', 'è¯•ç”¨æœŸè¿‡é•¿', 'éšæ—¶è§£é™¤',
            'ä¸äºˆè¡¥å¿', 'å•æ–¹é¢å†³å®š', 'æ‰¿æ‹…è´£ä»»', 'èµ”å¿æŸå¤±'
        ]
        
        medium_risk_indicators = [
            'å»ºè®®ä¿®æ”¹', 'éœ€è¦å®Œå–„', 'è¡¨è¿°ä¸æ¸…', 'å¯èƒ½å­˜åœ¨é£é™©', 'æ³¨æ„åˆè§„',
            'å»ºè®®è°ƒæ•´', 'éœ€è¦æ˜ç¡®', 'åº”å½“è§„èŒƒ', 'å¯èƒ½äº‰è®®', 'å»ºè®®ä¼˜åŒ–'
        ]
        
        low_risk_indicators = [
            'ç¬¦åˆæ³•å¾‹', 'è¡¨è¿°è§„èŒƒ', 'åŸºæœ¬åˆè§„', 'å»ºè®®ä¿æŒ', 'å†…å®¹åˆç†',
            'æ— é‡å¤§é—®é¢˜', 'åŸºæœ¬ç¬¦åˆ', 'è¾ƒä¸ºè§„èŒƒ'
        ]
        
        high_count = sum(1 for indicator in high_risk_indicators if indicator in text_lower)
        medium_count = sum(1 for indicator in medium_risk_indicators if indicator in text_lower)
        low_count = sum(1 for indicator in low_risk_indicators if indicator in text_lower)
        
        if high_count >= 2 or any(phrase in text_lower for phrase in ['ä¸¥é‡è¿æ³•', 'æ— æ•ˆæ¡æ¬¾', 'è´¥è¯‰']):
            return "é«˜é£é™©"
        elif high_count >= 1 or medium_count >= 2:
            return "ä¸­ç­‰é£é™©"
        elif medium_count >= 1 or (low_count == 0 and high_count == 0):
            return "ä¸­ç­‰é£é™©"
        else:
            return "ä½é£é™©"

    def _parse_analysis_fast(self, analysis: str) -> Dict[str, str]:
        result = {
            'defect_analysis': '',
            'related_cases': '',
            'judgment_result': '',
            'legal_basis': '',
            'modification_suggestion': ''
        }
        
        lines = analysis.split('\n')
        current_section = None
        
        for line in lines:
            line = line.strip()
            if 'æ¡æ¬¾ç¼ºé™·åˆ†æ' in line or 'ç¼ºé™·åˆ†æ' in line:
                current_section = 'defect_analysis'
                content = line.split('ï¼š', 1)[-1] if 'ï¼š' in line else ''
                if content:
                    result[current_section] = content
            elif 'ç›¸å…³æ¡ˆä¾‹' in line or 'æ¡ˆä¾‹' in line:
                current_section = 'related_cases'
                content = line.split('ï¼š', 1)[-1] if 'ï¼š' in line else ''
                if content:
                    result[current_section] = content
            elif 'åˆ¤å†³ç»“æœ' in line or 'åˆ¤å†³' in line:
                current_section = 'judgment_result'
                content = line.split('ï¼š', 1)[-1] if 'ï¼š' in line else ''
                if content:
                    result[current_section] = content
            elif 'æ³•å¾‹ä¾æ®' in line or 'æ³•æ¡' in line:
                current_section = 'legal_basis'
                content = line.split('ï¼š', 1)[-1] if 'ï¼š' in line else ''
                if content:
                    result[current_section] = content
            elif 'ä¿®æ”¹å»ºè®®' in line or 'å»ºè®®' in line:
                current_section = 'modification_suggestion'
                content = line.split('ï¼š', 1)[-1] if 'ï¼š' in line else ''
                if content:
                    result[current_section] = content
            elif line and current_section and not line.startswith(('æ¡æ¬¾', 'ç›¸å…³', 'åˆ¤å†³', 'æ³•å¾‹', 'ä¿®æ”¹')):
                if result[current_section]:
                    result[current_section] += '\n' + line
                else:
                    result[current_section] = line
        
        if not any(result.values()):
            result['defect_analysis'] = analysis
            result['modification_suggestion'] = 'è¯·æ ¹æ®åˆ†æç»“æœè¿›è¡Œç›¸åº”ä¿®æ”¹ã€‚'
        
        return result

    def _generate_smart_suggestions(self, clause_results: List[Dict]) -> List[Dict]:
        suggestions = []
        
        for i, result in enumerate(clause_results):
            risk_level = result.get('risk_level', 'ä½é£é™©')
            original_clause = result.get('input_clause', '')
            analysis = result.get('analysis', '')
            
            ai_suggestion = self._generate_ai_modification_suggestion(original_clause, analysis, risk_level)
            
            suggestion = {
                'clause_number': i + 1,
                'risk_level': risk_level,
                'original_clause': original_clause,
                'main_issues': result.get('defect_analysis', ''),
                'suggested_action': ai_suggestion.get('modification_action', result.get('modification_suggestion', '')),
                'related_cases': result.get('related_cases', ''),
                'judgment_result': result.get('judgment_result', ''),
                'legal_basis': result.get('legal_basis', ''),
                'priority': 1 if risk_level == 'é«˜é£é™©' else (2 if risk_level == 'ä¸­ç­‰é£é™©' else 3),
                'urgency': 'ç´§æ€¥' if risk_level == 'é«˜é£é™©' else ('é‡è¦' if risk_level == 'ä¸­ç­‰é£é™©' else 'ä¸€èˆ¬'),
                'ai_confidence': ai_suggestion.get('confidence', 'high'),
                'modification_type': ai_suggestion.get('modification_type', 'optimization'),
                'implementation_steps': ai_suggestion.get('implementation_steps', [])
            }
            
            suggestions.append(suggestion)
        
        suggestions = self._ai_prioritize_suggestions(suggestions)
        return suggestions

    def _generate_ai_modification_suggestion(self, clause_text: str, analysis: str, risk_level: str) -> Dict[str, Any]:
        
        if not hasattr(self.lora_trainer, 'peft_model') or self.lora_trainer.peft_model is None:
            raise RuntimeError("LoRAæ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•ç”ŸæˆAIä¿®æ”¹å»ºè®®")
        
        suggestion_prompt = f"""
åŸºäºä»¥ä¸‹åˆåŒæ¡æ¬¾åˆ†æç»“æœï¼Œè¯·ç”Ÿæˆå…·ä½“çš„ä¿®æ”¹å»ºè®®ï¼š

åŸå§‹æ¡æ¬¾ï¼š{clause_text}

åˆ†æç»“æœï¼š{analysis}

é£é™©ç­‰çº§ï¼š{risk_level}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼æä¾›ä¿®æ”¹å»ºè®®ï¼š
ä¿®æ”¹ç±»å‹ï¼š[åˆ é™¤/ä¿®æ”¹/è¡¥å……/é‡å†™]
å…·ä½“å»ºè®®ï¼š[è¯¦ç»†çš„ä¿®æ”¹å»ºè®®å’Œç†ç”±]
ä¿®æ”¹åæ¡æ¬¾ï¼š[æä¾›ä¿®æ”¹åçš„å…·ä½“æ¡æ¬¾æ–‡æœ¬]
å®æ–½æ­¥éª¤ï¼š[1. æ­¥éª¤ä¸€ 2. æ­¥éª¤äºŒ 3. æ­¥éª¤ä¸‰]
é¢„æœŸæ•ˆæœï¼š[ä¿®æ”¹åé¢„æœŸè¾¾åˆ°çš„æ•ˆæœ]
ç½®ä¿¡åº¦ï¼š[é«˜/ä¸­/ä½]
"""

        try:
            inputs = self.lora_trainer.tokenizer(
                suggestion_prompt,
                return_tensors="pt", 
                truncation=True, 
                max_length=self.performance_config['max_tokenizer_length']
            )
            
            # ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
            device = self.config['device']
            inputs = {k: v.to(device) for k, v in inputs.items()}

            with torch.no_grad():
                outputs = self.lora_trainer.peft_model.generate(
                    **inputs, 
                    max_new_tokens=self.performance_config['max_new_tokens'],
                    min_new_tokens=80,
                    do_sample=True,
                    temperature=0.6,
                    top_p=0.85,
                    repetition_penalty=1.2,
                    pad_token_id=self.lora_trainer.tokenizer.pad_token_id or self.lora_trainer.tokenizer.eos_token_id,
                    eos_token_id=self.lora_trainer.tokenizer.eos_token_id
                )
            
            input_length = inputs['input_ids'].shape[1]
            generated_tokens = outputs[0][input_length:]
            
            suggestion_text = self.lora_trainer.tokenizer.decode(
                generated_tokens,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=True
            ).strip()
            
            parsed_suggestion = self._parse_ai_suggestion(suggestion_text)
            
            print(f"AIæˆåŠŸç”Ÿæˆä¿®æ”¹å»ºè®®ï¼Œé•¿åº¦: {len(suggestion_text)} å­—ç¬¦")
            return parsed_suggestion
            
        except Exception as e:
            print(f"AIå»ºè®®ç”Ÿæˆå¤±è´¥: {e}")
            raise RuntimeError(f"å¤§æ¨¡å‹ä¿®æ”¹å»ºè®®ç”Ÿæˆå¤±è´¥: {e}")

    def _parse_ai_suggestion(self, suggestion_text: str) -> Dict[str, Any]:
        
        result = {
            'modification_type': 'optimization',
            'modification_action': '',
            'revised_clause': '',
            'implementation_steps': [],
            'expected_effect': '',
            'confidence': 'medium'
        }
        
        lines = suggestion_text.split('\n')
        current_section = None
        
        for line in lines:
            line = line.strip()
            if 'ä¿®æ”¹ç±»å‹ï¼š' in line or 'ä¿®æ”¹ç±»å‹:' in line:
                mod_type = line.split('ï¼š', 1)[-1] if 'ï¼š' in line else line.split(':', 1)[-1]
                result['modification_type'] = mod_type.strip()
            elif 'å…·ä½“å»ºè®®ï¼š' in line or 'å…·ä½“å»ºè®®:' in line:
                current_section = 'modification_action'
                content = line.split('ï¼š', 1)[-1] if 'ï¼š' in line else line.split(':', 1)[-1]
                if content.strip():
                    result[current_section] = content.strip()
            elif 'ä¿®æ”¹åæ¡æ¬¾ï¼š' in line or 'ä¿®æ”¹åæ¡æ¬¾:' in line:
                current_section = 'revised_clause'
                content = line.split('ï¼š', 1)[-1] if 'ï¼š' in line else line.split(':', 1)[-1]
                if content.strip():
                    result[current_section] = content.strip()
            elif 'å®æ–½æ­¥éª¤ï¼š' in line or 'å®æ–½æ­¥éª¤:' in line:
                current_section = 'implementation_steps'
                content = line.split('ï¼š', 1)[-1] if 'ï¼š' in line else line.split(':', 1)[-1]
                if content.strip():
                    result[current_section] = [content.strip()]
            elif 'é¢„æœŸæ•ˆæœï¼š' in line or 'é¢„æœŸæ•ˆæœ:' in line:
                current_section = 'expected_effect'
                content = line.split('ï¼š', 1)[-1] if 'ï¼š' in line else line.split(':', 1)[-1]
                if content.strip():
                    result[current_section] = content.strip()
            elif 'ç½®ä¿¡åº¦ï¼š' in line or 'ç½®ä¿¡åº¦:' in line:
                confidence = line.split('ï¼š', 1)[-1] if 'ï¼š' in line else line.split(':', 1)[-1]
                result['confidence'] = confidence.strip().lower()
            elif line and current_section:
                if current_section == 'implementation_steps':
                    if line.startswith(('1.', '2.', '3.', '4.', '5.')):
                        result[current_section].append(line)
                    elif result[current_section]:
                        result[current_section][-1] += '\n' + line
                else:
                    if result[current_section]:
                        result[current_section] += '\n' + line
                    else:
                        result[current_section] = line
        
        if not result['modification_action']:
            result['modification_action'] = suggestion_text[:200] + ('...' if len(suggestion_text) > 200 else '')
        
        return result

    def _ai_prioritize_suggestions(self, suggestions: List[Dict]) -> List[Dict]:
        
        if not hasattr(self.lora_trainer, 'peft_model') or self.lora_trainer.peft_model is None:
            return sorted(suggestions, key=lambda x: (x['priority'], x['clause_number']))
        
        try:
            suggestion_summary = []
            for i, sug in enumerate(suggestions):
                summary = f"æ¡æ¬¾{sug['clause_number']}: {sug['risk_level']}, é—®é¢˜: {sug['main_issues'][:50]}..."
                suggestion_summary.append(summary)
            
            priority_prompt = f"""
è¯·å¯¹ä»¥ä¸‹åˆåŒæ¡æ¬¾ä¿®æ”¹å»ºè®®è¿›è¡Œä¼˜å…ˆçº§æ’åºï¼Œè€ƒè™‘æ³•å¾‹é£é™©ã€ç´§æ€¥ç¨‹åº¦ã€å®æ–½éš¾åº¦ç­‰å› ç´ ï¼š

{chr(10).join(suggestion_summary)}

è¯·æŒ‰ä¼˜å…ˆçº§ä»é«˜åˆ°ä½æ’åºï¼Œè¾“å‡ºæ ¼å¼ï¼š
ä¼˜å…ˆçº§æ’åºï¼šæ¡æ¬¾X, æ¡æ¬¾Y, æ¡æ¬¾Z...
æ’åºç†ç”±ï¼š[ç®€è¦è¯´æ˜æ’åºä¾æ®]
"""

            inputs = self.lora_trainer.tokenizer(
                priority_prompt,
                return_tensors="pt", 
                truncation=True, 
                max_length=400
            )
            
            device = self.config['device']
            inputs = {k: v.to(device) for k, v in inputs.items()}

            with torch.no_grad():
                outputs = self.lora_trainer.peft_model.generate(
                    **inputs, 
                    max_new_tokens=100,
                    do_sample=True,
                    temperature=0.5,
                    top_p=0.8
                )
            
            input_length = inputs['input_ids'].shape[1]
            priority_result = self.lora_trainer.tokenizer.decode(
                outputs[0][input_length:], skip_special_tokens=True
            ).strip()
            
            ai_order = self._parse_ai_priority_order(priority_result, len(suggestions))
            
            if ai_order:
                reordered_suggestions = []
                for clause_num in ai_order:
                    for sug in suggestions:
                        if sug['clause_number'] == clause_num:
                            reordered_suggestions.append(sug)
                            break
                
                for sug in suggestions:
                    if sug not in reordered_suggestions:
                        reordered_suggestions.append(sug)
                
                print(f"AIæ™ºèƒ½æ’åºå®Œæˆï¼Œä¼˜å…ˆçº§: {ai_order}")
                return reordered_suggestions
            
        except Exception as e:
            print(f"AIä¼˜å…ˆçº§æ’åºå¤±è´¥: {e}")
        
        return sorted(suggestions, key=lambda x: (x['priority'], x['clause_number']))

    def _parse_ai_priority_order(self, priority_text: str, total_suggestions: int) -> List[int]:
        
        import re
        
        for line in priority_text.split('\n'):
            if 'ä¼˜å…ˆçº§æ’åº' in line or 'æ’åº' in line:
                numbers = re.findall(r'æ¡æ¬¾(\d+)', line)
                if numbers:
                    try:
                        return [int(num) for num in numbers if 1 <= int(num) <= total_suggestions]
                    except ValueError:
                        continue
        
        return []

    def _generate_contract_summary_fast(self, clauses: List[str], clause_results: List[Dict]) -> str:
        risk_counts = pd.Series([r['risk_level'] for r in clause_results]).value_counts().to_dict()
        
        high_risk = risk_counts.get('é«˜é£é™©', 0)
        medium_risk = risk_counts.get('ä¸­ç­‰é£é™©', 0)
        low_risk = risk_counts.get('ä½é£é™©', 0)
        
        main_issues = []
        for i, result in enumerate(clause_results, 1):
            if result.get('risk_level') in ['é«˜é£é™©', 'ä¸­ç­‰é£é™©']:
                defect = result.get('defect_analysis', '').split('\n')[0][:80]
                if defect:
                    main_issues.append(f"æ¡æ¬¾{i}: {defect}")
        
        summary = f"""
ã€AIæ™ºèƒ½åˆ†ææ‘˜è¦ã€‘
âœ… æ¡æ¬¾æ€»æ•°: {len(clauses)} ä¸ª
ğŸ”´ é«˜é£é™©æ¡æ¬¾: {high_risk} ä¸ª
ğŸŸ¡ ä¸­ç­‰é£é™©æ¡æ¬¾: {medium_risk} ä¸ª  
ğŸŸ¢ ä½é£é™©æ¡æ¬¾: {low_risk} ä¸ª

ã€ä¸»è¦é£é™©ç‚¹ã€‘
{chr(10).join(main_issues[:5]) if main_issues else 'âœ… æœªå‘ç°é‡å¤§é£é™©ç‚¹'}

ã€AIåˆ†æå»ºè®®ã€‘
åŸºäºå¤§æ¨¡å‹ä¸“ä¸šåˆ†æï¼Œå»ºè®®é‡ç‚¹å…³æ³¨é«˜é£é™©æ¡æ¬¾ï¼ŒåŠæ—¶ä¿®è®¢ä»¥é™ä½æ³•å¾‹é£é™©ã€‚
        """.strip()
        
        return summary

    def _assess_contract_risks_enhanced(self, clause_results: List[Dict]) -> Dict[str, Any]:
        
        total_clauses = len(clause_results)
        high_risk = len([r for r in clause_results if r.get('risk_level') == 'é«˜é£é™©'])
        medium_risk = len([r for r in clause_results if r.get('risk_level') == 'ä¸­ç­‰é£é™©'])
        low_risk = len([r for r in clause_results if r.get('risk_level') == 'ä½é£é™©'])
        
        risk_score = (high_risk * 25 + medium_risk * 10 + low_risk * 2) / total_clauses if total_clauses > 0 else 0
        
        if risk_score >= 20:
            overall_risk, risk_color = "é«˜é£é™©", "#d32f2f"
        elif risk_score >= 10:
            overall_risk, risk_color = "ä¸­ç­‰é£é™©", "#f57c00"
        else:
            overall_risk, risk_color = "ä½é£é™©", "#388e3c"
        
        return {
            'overall_risk_level': overall_risk,
            'risk_score': round(risk_score, 2),
            'risk_color': risk_color,
            'risk_distribution': {
                'high_risk': high_risk,
                'medium_risk': medium_risk,
                'low_risk': low_risk,
                'total': total_clauses
            },
            'risk_percentage': {
                'high': round(high_risk / total_clauses * 100, 1) if total_clauses > 0 else 0,
                'medium': round(medium_risk / total_clauses * 100, 1) if total_clauses > 0 else 0,
                'low': round(low_risk / total_clauses * 100, 1) if total_clauses > 0 else 0
            },
            'risk_intensity': 'severe' if risk_score >= 20 else 'moderate' if risk_score >= 10 else 'mild',
            'ai_confidence': 'high'
        }

    def _build_comprehensive_result(self, contract_text, clauses, clause_results, summary, risk, suggestions, time, mode):
        return {
            'contract_text': contract_text,
            'clause_count': len(clauses),
            'contract_summary': summary,
            'risk_assessment': risk,
            'modification_suggestions': suggestions,
            'processing_time': time,
            'extracted_clauses': clauses,
            'clause_results': clause_results,
            'analysis_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'performance_mode': mode
        }
        
    def _create_fallback_result(self, contract_text, start_time, error_msg):
        return {
            'error': error_msg,
            'contract_text': contract_text,
            'clause_count': 0,
            'processing_time': time.time() - start_time,
            'analysis_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'status': 'failed'
        }
        
    def _create_fallback_clause_result(self, clause_text, index):
        return {
            'input_clause': clause_text,
            'risk_level': 'åˆ†æå¤±è´¥',
            'analysis': f'æ¡æ¬¾åˆ†æå¤±è´¥ï¼Œé”™è¯¯ç´¢å¼•: {index}',
            'rag_sources': 0,
            'clause_index': index,
            'error': 'analysis_failed'
        }

rag_system = LegalRAGSystem()
lora_trainer = LegalLoRATrainer()
integrated_analyzer = IntegratedLegalAnalyzer(rag_system, lora_trainer)
analyzer = OptimizedContractAnalyzer(integrated_analyzer)




def check_model_cache_status() -> Dict[str, Any]:
    try:
        status = {
            'mkl_threading_layer': os.environ.get('MKL_THREADING_LAYER', ''),
            'vector_backend': 'sklearn',
            'gpu_available': GPU_AVAILABLE,
            'gpu_memory_gb': round(GPU_MEMORY_GB, 2) if GPU_AVAILABLE else 0,
            'embedding_model': LINUX_CONFIG['rag_config'].get('embedding_model'),
            'model_dir_exists': os.path.isdir('./legal_lora_model'),
            'rag_index': {
                'knowledge': os.path.exists('legal_rag_index_knowledge.pkl'),
                'embeddings': os.path.exists('legal_rag_index_embeddings.npy'),
                # 'faiss_index' removed; sklearn backend does not use an index
            }
        }
        return status
    except Exception as e:
        return {'error': str(e)}


def initialize_models():
    if not rag_system.load_index():
        print("RAGç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
    lora_trainer.load_model_and_tokenizer() 